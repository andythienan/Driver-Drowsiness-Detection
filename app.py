import warnings
from collections import deque
from typing import Deque, Optional

import av
import joblib
import mediapipe as mp
import numpy as np
import streamlit as st
from PIL import Image, ImageDraw
from streamlit_webrtc import WebRtcMode, VideoProcessorBase, webrtc_streamer


# Suppress non-critical warnings (similar to original script)
warnings.filterwarnings("ignore")


# -----------------------------
# 1. CONSTANTS & GEOMETRY UTILS
# -----------------------------

# Landmark indices (same as in your original run_webcam.py)
LEFT_EYE_EAR_INDICES = [33, 160, 158, 133, 153, 144]
RIGHT_EYE_EAR_INDICES = [362, 385, 387, 263, 373, 380]
MOUTH_MAR_INDICES = [61, 82, 13, 312, 291, 317, 14, 87]


def calculate_ear(eye_points):
    p = [np.array([point.x, point.y]) for point in eye_points]
    vertical_1 = np.linalg.norm(p[1] - p[5])
    vertical_2 = np.linalg.norm(p[2] - p[4])
    numerator = vertical_1 + vertical_2
    horizontal = np.linalg.norm(p[0] - p[3])
    denominator = 2.0 * horizontal
    if denominator == 0:
        return 0.0
    return numerator / denominator


def calculate_mar(mouth_points):
    p = [np.array([point.x, point.y]) for point in mouth_points]
    vertical_1 = np.linalg.norm(p[1] - p[7])
    vertical_2 = np.linalg.norm(p[2] - p[6])
    vertical_3 = np.linalg.norm(p[3] - p[5])
    numerator = vertical_1 + vertical_2 + vertical_3
    horizontal = np.linalg.norm(p[0] - p[4])
    denominator = 2.0 * horizontal
    if denominator == 0:
        return 0.0
    return numerator / denominator


# -----------------------------
# 2. MODEL & SCALER LOADING
# -----------------------------


@st.cache_resource
def load_model_and_scaler():
    """
    Load the SVM model and scaler once and reuse across sessions.
    """
    try:
        model = joblib.load("svm_model.joblib")
        scaler = joblib.load("scaler.joblib")
    except FileNotFoundError:
        st.error("Kh√¥ng t√¨m th·∫•y file `svm_model.joblib` ho·∫∑c `scaler.joblib`. "
                 "H√£y ƒë·∫£m b·∫£o ƒë·∫∑t ch√∫ng c√πng th∆∞ m·ª•c v·ªõi `app.py`.")
        st.stop()

    return model, scaler


model, scaler = load_model_and_scaler()


# -----------------------------
# 3. VIDEO PROCESSOR CLASS
# -----------------------------


class DrowsinessVideoProcessor(VideoProcessorBase):
    """
    VideoProcessor for streamlit-webrtc that:
    - Uses MediaPipe Face Mesh to get landmarks
    - Computes EAR (both eyes) and MAR (mouth)
    - Uses your trained SVM + scaler for drowsiness classification
    - Overlays status text on each frame
    """

    def __init__(
        self,
        smoothing_window: int = 25,
        fatigue_prob_threshold: float = 50.0,
    ):
        self.smoothing_window = max(1, int(smoothing_window))
        self.fatigue_prob_threshold = float(fatigue_prob_threshold)

        # Buffer for smoothing predictions
        self.prediction_buffer: Deque[int] = deque(maxlen=self.smoothing_window)

        # MediaPipe Face Mesh initialization
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=False,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5,
        )

        # For optional charting
        self.current_ear: Optional[float] = None
        self.current_mar: Optional[float] = None
        self.current_label: Optional[int] = None
        self.current_confidence: Optional[float] = None

        # Internal alarm flag (visual only here; audio not used in web app)
        self.alarm_sounding = False

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        # Get image in BGR format and flip horizontally (mirror effect)
        image_bgr = frame.to_ndarray(format="bgr24")
        image_bgr = np.ascontiguousarray(image_bgr[:, ::-1, :])

        # Convert to RGB for MediaPipe
        rgb_frame = image_bgr[:, :, ::-1]
        results = self.face_mesh.process(rgb_frame)

        status_text = "KHONG PHAT HIEN KHUON MAT"
        status_color = (0, 0, 255)  # Red

        if results.multi_face_landmarks:
            face_landmarks = results.multi_face_landmarks[0].landmark

            try:
                # --- Feature extraction (same as original script) ---
                left_eye_points = [face_landmarks[i] for i in LEFT_EYE_EAR_INDICES]
                right_eye_points = [face_landmarks[i] for i in RIGHT_EYE_EAR_INDICES]
                avg_ear = (calculate_ear(left_eye_points) + calculate_ear(right_eye_points)) / 2.0

                mouth_points = [face_landmarks[i] for i in MOUTH_MAR_INDICES]
                mar = calculate_mar(mouth_points)

                self.current_ear = float(avg_ear)
                self.current_mar = float(mar)

                # --- 2-feature vector & scaling ---
                features = [avg_ear, mar]
                features_scaled = scaler.transform([features])

                # --- Prediction ---
                prediction = model.predict(features_scaled)
                probability = model.predict_proba(features_scaled)
                label = int(prediction[0])  # 0 = ACTIVE, 1 = FATIGUE (same convention)

                # Confidence for this label (as percentage)
                confidence = float(probability[0][label] * 100.0)
                self.current_label = label
                self.current_confidence = confidence

                # --- Smoothing ---
                self.prediction_buffer.append(label)
                try:
                    final_label = int(np.bincount(self.prediction_buffer).argmax())
                except ValueError:
                    final_label = label

                # Thresholding by probability if user set > 50%
                is_fatigued = (
                    final_label == 1 and confidence >= self.fatigue_prob_threshold
                )

                if not is_fatigued:
                    status_text = f"ACTIVE ({confidence:.1f}%)"
                    status_color = (0, 255, 0)  # Green
                    self.alarm_sounding = False
                else:
                    status_text = f"FATIGUE ({confidence:.1f}%)"
                    status_color = (0, 0, 255)  # Red
                    # In a browser environment we avoid OS-level beeps
                    self.alarm_sounding = True

            except Exception:
                status_text = "LOI TRICH XUAT"
                status_color = (0, 0, 255)

        # Overlay status text using Pillow on RGB image
        overlay_rgb = rgb_frame.copy()
        pil_img = Image.fromarray(overlay_rgb)
        draw = ImageDraw.Draw(pil_img)

        # Convert BGR color to RGB for Pillow
        text_color_rgb = (status_color[2], status_color[1], status_color[0])
        draw.text((50, 50), status_text, fill=text_color_rgb)

        overlay_rgb = np.array(pil_img)
        # Convert back to BGR for WebRTC output
        output_bgr = overlay_rgb[:, :, ::-1]

        return av.VideoFrame.from_ndarray(output_bgr, format="bgr24")

    def update_params(self, smoothing_window: int, fatigue_prob_threshold: float):
        """
        Update smoothing window and probability threshold from sidebar controls.
        """
        smoothing_window = max(1, int(smoothing_window))
        if smoothing_window != self.smoothing_window:
            self.smoothing_window = smoothing_window
            # Recreate buffer with new size
            self.prediction_buffer = deque(self.prediction_buffer, maxlen=self.smoothing_window)

        self.fatigue_prob_threshold = float(fatigue_prob_threshold)


# -----------------------------
# 4. STREAMLIT UI
# -----------------------------


def main():
    st.set_page_config(
        page_title="Real-time Drowsiness Detection",
        page_icon="üò¥",
        layout="wide",
    )

    st.title("üöó Real-time Driver Drowsiness Detection")
    st.markdown(
        """
        ·ª®ng d·ª•ng web s·ª≠ d·ª•ng **MediaPipe Face Mesh** v√† **SVM** ƒë·ªÉ ph√°t hi·ªán bu·ªìn ng·ªß theo th·ªùi gian th·ª±c.
        
        - M√¥ h√¨nh: SVM (ƒë√£ hu·∫•n luy·ªán tr∆∞·ªõc) d√πng 2 ƒë·∫∑c tr∆∞ng: EAR (Eye Aspect Ratio) & MAR (Mouth Aspect Ratio).
        - Video stream ƒë∆∞·ª£c x·ª≠ l√Ω b·∫±ng **streamlit-webrtc** ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªô tr·ªÖ th·∫•p v√† UI m∆∞·ª£t.
        """
    )

    # ---- Sidebar controls ----
    st.sidebar.header("‚öôÔ∏è C√†i ƒë·∫∑t")

    smoothing_window = st.sidebar.slider(
        "ƒê·ªô m∆∞·ª£t d·ª± ƒëo√°n (s·ªë khung h√¨nh)",
        min_value=5,
        max_value=50,
        value=25,
        step=1,
        help="S·ª≠ d·ª•ng trung b√¨nh s·ªë khung h√¨nh n√†y ƒë·ªÉ l√†m m∆∞·ª£t d·ª± ƒëo√°n (gi·∫£m nhi·ªÖu).",
    )

    fatigue_prob_threshold = st.sidebar.slider(
        "Ng∆∞·ª°ng x√°c su·∫•t bu·ªìn ng·ªß (%)",
        min_value=50.0,
        max_value=99.0,
        value=70.0,
        step=1.0,
        help="N·∫øu x√°c su·∫•t m√¥ h√¨nh > ng∆∞·ª°ng n√†y v√† nh√£n l√† FATIGUE th√¨ coi l√† bu·ªìn ng·ªß.",
    )

    st.sidebar.markdown("---")
    st.sidebar.markdown("**Tr·∫°ng th√°i hi·ªán t·∫°i** s·∫Ω ƒë∆∞·ª£c hi·ªÉn th·ªã tr√™n video.")

    # ---- Main content layout ----
    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader("üì∑ Webcam")

        # WebRTC configuration for public deployment (STUN server)
        rtc_configuration = {
            "iceServers": [
                {"urls": ["stun:stun.l.google.com:19302"]},
            ]
        }

        webrtc_ctx = webrtc_streamer(
            key="drowsiness-detection",
            mode=WebRtcMode.SENDRECV,
            rtc_configuration=rtc_configuration,
            media_stream_constraints={"video": True, "audio": False},
            video_processor_factory=lambda: DrowsinessVideoProcessor(
                smoothing_window=smoothing_window,
                fatigue_prob_threshold=fatigue_prob_threshold,
            ),
        )

    with col2:
        st.subheader("üìä Th√¥ng s·ªë th·ªùi gian th·ª±c")
        ear_placeholder = st.metric("EAR (Eye Aspect Ratio)", value="-")
        mar_placeholder = st.metric("MAR (Mouth Aspect Ratio)", value="-")
        status_placeholder = st.empty()

        ear_chart = st.empty()

        # Store history in session_state for charting
        if "ear_history" not in st.session_state:
            st.session_state["ear_history"] = []

        # Real-time stats / chart loop (polling)
        # Note: Streamlit reruns the script automatically; no infinite loop here.
        if webrtc_ctx and webrtc_ctx.video_processor:
            processor: DrowsinessVideoProcessor = webrtc_ctx.video_processor

            # Sync sidebar parameters with processor (in case user changed them)
            processor.update_params(
                smoothing_window=smoothing_window,
                fatigue_prob_threshold=fatigue_prob_threshold,
            )

            if processor.current_ear is not None:
                ear_placeholder.metric(
                    "EAR (Eye Aspect Ratio)", f"{processor.current_ear:.3f}"
                )
                st.session_state["ear_history"].append(processor.current_ear)

            if processor.current_mar is not None:
                mar_placeholder.metric(
                    "MAR (Mouth Aspect Ratio)", f"{processor.current_mar:.3f}"
                )

            # Status text
            if processor.current_label is not None and processor.current_confidence is not None:
                if processor.current_label == 0:
                    status_placeholder.markdown(
                        "<span style='color:limegreen; font-size:24px; font-weight:bold;'>‚úÖ ACTIVE</span>",
                        unsafe_allow_html=True,
                    )
                else:
                    status_placeholder.markdown(
                        "<span style='color:red; font-size:24px; font-weight:bold;'>‚ö†Ô∏è DROWSY!</span>",
                        unsafe_allow_html=True,
                    )

            # Real-time EAR chart (optional)
            if len(st.session_state["ear_history"]) > 1:
                ear_chart.line_chart(st.session_state["ear_history"])
        else:
            status_placeholder.info(
                "B·∫≠t webcam ·ªü ph·∫ßn b√™n tr√°i ƒë·ªÉ xem th√¥ng s·ªë v√† tr·∫°ng th√°i."
            )


if __name__ == "__main__":
    main()


